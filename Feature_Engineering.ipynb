{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7iFXh0sg86w"
      },
      "outputs": [],
      "source": [
        "#Q What is a parameter?\n",
        "#  A parameter in machine learning refers to the internal variables of a model that are learned from data during training. These are the values the model adjusts to make accurate predictions.\n",
        "#  Example-- 1. Linear Regression: Equation: y = wx +\n",
        "#                                  Parameters: w (weight) and b (bias)\n",
        "\n",
        "#            2. Neural Networks:Parameters include weights and biases for each neuron.\n",
        "#                               A large model can have millions of parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What is correlation? What does negative correlation mean?\n",
        "#  Correlation measures the relationship between two variables — specifically, how they move in relation to each other. It answers: \"When one variable changes, what tends to happen to the other?\"\n",
        "#  Types of Correlation: Positive, Negative, Zero\n",
        "#  What Does Negative Correlation Mean-- A negative correlation means that as one variable increases, the other decreases.\n",
        "#                                        Example: The more hours you study, the fewer mistakes you make on a test."
      ],
      "metadata": {
        "id": "MIJnnpQJhrDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q Define Machine Learning. What are the main components in Machine Learning?\n",
        "#  Machine Learning is a branch of Artificial Intelligence (AI) where computers learn from data without being explicitly programmed. Instead of writing rules, you feed data to a model, and it learns patterns to make predictions or decisions.\n",
        "#  Main Components of Machine Learning:\n",
        "#    1. Data-- Raw input: numbers, images, text, etc. The quality and quantity of data directly affect the performance of the model.\n",
        "#    2. Features-- The individual measurable properties or characteristics of the data. Example: For house pricing, features could be square footage, location, number of rooms.\n",
        "#    3. Parameters The internal variables the model learns during training. Example: Weights in a neural network.\n",
        "#    4. Training-- The process where the model learns from the data by adjusting parameters to reduce error.\n",
        "#    5. Model-- The algorithm or mathematical structure that learns from the data. Examples: Linear regression, decision trees, neural networks.\n",
        "#    6. Evaluation/Testing-- Testing the model on new (unseen) data to check its performance. Common metrics: Accuracy, Precision, Recall, F1 Score, etc."
      ],
      "metadata": {
        "id": "fs9sPDkLi_Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q How does loss value help in determining whether the model is good or not?\n",
        "#  1. Measures Error-- The loss gives a direct measure of how bad the model is doing at that moment.\n",
        "#                      It’s what the model tries to minimize during training.\n",
        "#  2. Guides Training-- The optimizer uses the loss value to know how to adjust the model’s parameters.\n",
        "#                      Think of it like a compass pointing toward better performance.\n",
        "#  3. Monitoring Progress-- If loss is decreasing over time → the model is learning.\n",
        "#                          If loss is stuck or increasing → something might be wrong (bad data, wrong learning rate, etc.).\n",
        "#  4. Doesn’t Always Tell the Full Story-- A low loss on training data doesn’t guarantee the model will perform well on new data.\n",
        "#                          It might be overfitting — learning the training data too well but failing on real-world inputs.\n",
        "#                          That’s why we also check validation/test loss and use evaluation metrics."
      ],
      "metadata": {
        "id": "IZwKoV1xkHdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What are continuous and categorical variables?\n",
        "#  1. Continuous Variables-- These are numerical values that can take on any value within a range — including decimals.\n",
        "#                             Examples:Age: 23.5 years, Height: 170.2 cm, Temperature: 36.6°C, Income: $45,300\n",
        "#  2. Categorical Variables-- These are variables that represent categories or groups. They don't have a numerical meaning, even if they’re coded as numbers.\n",
        "#                             Examples: Gender: Male, Female, Other,, Color: Red, Blue, Green,, Country: India, USA, Brazil,, Rating: Low, Medium, High"
      ],
      "metadata": {
        "id": "q_OZJlujku7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "#  Machine learning models can’t directly process text like \"Red\" or \"Male\" — we need to convert these categories into numbers in a meaningful way.\n",
        "#  Common Techniques to Handle Categorical Variables:\n",
        "#  1. Label Encoding-- Assigns a unique number to each category.\n",
        "#  2. One-Hot Encoding-- Creates a new binary column for each category. Value is 1 if that category is present, 0 otherwise.\n",
        "#  3. Ordinal Encoding-- Like label encoding but with meaningful order.\n",
        "#  4. Target Encoding (Mean Encoding)-- Replace each category with the average of the target variable for that category.\n",
        "#  5. Frequency Encoding-- Replace each category with its frequency in the dataset."
      ],
      "metadata": {
        "id": "SoOH4MFrldbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What do you mean by training and testing a dataset?\n",
        "#  1. Training Set-- The training set is the portion of the data used to teach the model. The model learns patterns in this data by adjusting internal parameters (like weights).\n",
        "#                    Goal: Help the model understand the relationship between input and output.\n",
        "#  2. Testing Set-- The testing set is separate data that the model has never seen before. It is used to evaluate how well the model can generalize to new, unseen data.\n",
        "#                   Goal: See if the model performs well in the real world, not just on the data"
      ],
      "metadata": {
        "id": "lwbZ9LjNmrOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What is sklearn.preprocessing?\n",
        "#  sklearn.preprocessing is a module in Scikit-learn (a popular Python machine learning library) that contains tools to prepare and transform your data before feeding it into a machine learning model."
      ],
      "metadata": {
        "id": "yytRfJEFnG9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What is a Test set?\n",
        "#  A test set is a portion of your dataset that is used to evaluate your machine learning model after it has been trained.\n",
        "#  The model has never seen the test set during training."
      ],
      "metadata": {
        "id": "nMxY7xQonXlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "#  In Python, we typically use train_test_split from Scikit-learn to split the data into training and testing sets. This allows us to train a model on one portion of the data and test its performance on another portion.\n",
        "#  Steps to Split Data:\n",
        "#  1. Import necessary libraries: Scikit-learn provides the train_test_split function to easily split data.\n",
        "#  2. Split your data: You can define the percentage of data used for training and testing (commonly 80%/20% or 70%/30%).\n",
        "\n",
        "#  Approach a Machine Learning Problem--\n",
        "#  1. Define the Problem\n",
        "#  2. Collect and Prepare Data\n",
        "#  3. Preprocess Data\n",
        "#  4. Select a Model\n",
        "#  5. Train the Model\n",
        "#  6. Evaluate the Model\n",
        "#  7. Improve the Model\n",
        "#  8.  Deployment and Monitoring (Final Step)"
      ],
      "metadata": {
        "id": "kkJDHhq9nhmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q Why do we have to perform EDA before fitting a model to the data?\n",
        "#   Exploratory Data Analysis (EDA) is a crucial step in the machine learning pipeline. It’s the process of analyzing and understanding your data before fitting a model.\n",
        "#   Why Perform EDA Before Fitting a Model-- Understanding the Data, Data Quality Check, Feature Understanding & Engineering, Choosing the Right Model, Detecting Biases, Feature Scaling Decision"
      ],
      "metadata": {
        "id": "joo8S0-eo9Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What is correlation?\n",
        "#  Correlation is a statistical measure that describes the strength and direction of the relationship between two variables. In simpler terms, it tells you how two variables change together."
      ],
      "metadata": {
        "id": "3GDT_Tsgpj3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What does negative correlation mean?\n",
        "#  A negative correlation means that as one variable increases, the other variable decreases. In other words, the two variables move in opposite directions.\n",
        "#  Key Points of Negative Correlation: When one variable goes up, the other goes down.\n",
        "#                                      The relationship is inverse or opposite.\n",
        "#                                      The correlation coefficient r for negative correlation lies between -1 and 0."
      ],
      "metadata": {
        "id": "ul5ZnMcVp0mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q How can you find correlation between variables in Python?\n",
        "#  To find the correlation between variables in Python, you typically use Pandas and Seaborn/Matplotlib for data manipulation and visualization.\n",
        "#   We can compute correlation using the .corr() function in Pandas, and then visualize it using a heatmap or scatter plot."
      ],
      "metadata": {
        "id": "SsVT3nL-qE26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What is causation? Explain difference between correlation and causation with an example.\n",
        "#  Causation refers to a relationship where one event or variable directly causes the occurrence or change of another event or variable. In simple terms, causation means that one thing makes something else happen.\n",
        "#  Difference Between Correlation and Causation:\n",
        "#      1. Correlation refers to a statistical relationship between two variables. When two variables are correlated, it means they tend to change in some way together (either positively or negatively).\n",
        "#         correlation does not imply causation. Just because two variables are correlated doesn't mean one causes the other. Correlation can occur due to a third variable that influences both correlated variables.\n",
        "#      2. Causation: Causation means that one variable directly influences or causes a change in the other.\n",
        "#         Establishing causation often requires experimental or longitudinal data where one variable is manipulated and the effect on the other variable is observed."
      ],
      "metadata": {
        "id": "jQdLiQaoqWh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "#  An optimizer in machine learning is an algorithm or method used to adjust the weights of the model during training in order to minimize the loss function. The goal of an optimizer is to find the best parameters (weights) that make the model's predictions as accurate as possible.\n",
        "#  Types of Optimizers:\n",
        "#  1. Stochastic Gradient Descent (SGD): In standard gradient descent, the entire dataset is used to calculate the gradient and update the weights. In Stochastic Gradient Descent (SGD), only one data point is used to calculate the gradient and update the weights at each step, making it faster and more efficient.\n",
        "#     Example- from sklearn.linear_model import SGDClassifier\n",
        "#              model = SGDClassifier(loss=\"log\", max_iter=1000)\n",
        "#              model.fit(X_train, y_train)\n",
        "#  2. Mini-Batch Gradient Descent: A compromise between batch gradient descent and stochastic gradient descent. Instead of using the entire dataset (batch) or just one data point (stochastic), mini-batch gradient descent uses a subset (mini-batch) of the data to compute the gradient and update the weights.\n",
        "#                                  This speeds up training while maintaining some level of stability.\n",
        "#     Example- from sklearn.linear_model import SGDClassifier\n",
        "#              model = SGDClassifier(loss=\"log\", max_iter=1000, batch_size=32)\n",
        "#              model.fit(X_train, y_train)\n",
        "#  3. Momentum: Momentum is an extension of gradient descent that helps accelerate the gradient vectors in the right directions, thus leading to faster converging.\n",
        "#               It does this by considering the previous gradient updates and adding them to the current update, smoothing the direction of movement.\n",
        "#     Example- from sklearn.linear_model import SGDClassifier\n",
        "#              model = SGDClassifier(loss=\"log\", max_iter=1000, momentum=0.9)\n",
        "#              model.fit(X_train, y_train)\n",
        "#  4. Nesterov Accelerated Gradient (NAG): A more advanced version of momentum. Nesterov's method looks ahead of the current gradient update, calculating the gradient at the \"look-ahead\" point, leading to more accurate and faster updates.\n",
        "#     Example- from keras.optimizers import SGD\n",
        "#              optimizer = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
        "#              model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "CvoLXG4qq4Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What is sklearn.linear_model ?\n",
        "#  sklearn.linear_model is a module within the scikit-learn library that provides various linear models for machine learning tasks, specifically for regression and classification problems.\n",
        "#  These models assume a linear relationship between the input features and the target variable. They are widely used because of their simplicity, interpretability, and efficiency."
      ],
      "metadata": {
        "id": "h5NUnYt2sZpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What does model.fit() do? What arguments must be given?\n",
        "#  The model.fit() method in machine learning is used to train the model. It takes the training data and the target labels, and it adjusts the model's internal parameters (like weights in a neural network or coefficients in linear regression) to learn from the data.\n",
        "#  The model learns the relationship between the features (inputs) and the target (output) by minimizing the loss function.\n",
        "\n",
        "#  Arguments to model.fit():\n",
        "#  1. X_train (Required): This is the input data (features) for training. It is typically a 2D array (or matrix), where each row is an observation and each column is a feature.\n",
        "#     Example: If you're training on a dataset of house prices, X_train might include columns for features like square footage, number of bedrooms, etc.\n",
        "#  2. y_train (Required): This is the target data (labels or outputs) corresponding to the input data X_train. In supervised learning, y_train contains the correct output values.\n",
        "#     Example: If you're predicting house prices, y_train would be a 1D array with the actual prices for the houses in the training set."
      ],
      "metadata": {
        "id": "2qrF3eG9si9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What does model.predict() do? What arguments must be given?\n",
        "#  The model.predict() method in machine learning is used to make predictions after a model has been trained using the fit() method. It generates output (predicted values or labels) based on the input data provided to it.\n",
        "#  Arguments to model.predict():\n",
        "#  X (Required): This is the input data for which you want to make predictions. It should be in the same format as the training data X_train that was used in model.fit(). For example, a 2D array where each row represents a sample and each column represents a feature.\n",
        "#   Shape: (n_samples, n_features), where n_samples is the number of data points (instances) for which you want predictions, and n_features is the number of features per instance.\n",
        "#   Example: If you have a dataset of house prices and you want to predict the price of new houses, X would contain features like square footage, number of bedrooms, etc. for those new houses."
      ],
      "metadata": {
        "id": "DaXuOB71tQd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What are continuous and categorical variables?\n",
        "#  1. Continuous Variables-- Continuous variables are numeric variables that can take an infinite number of values within a given range. These values can be measured and can represent quantities with decimals or fractions.\n",
        "#     Examples: Height, Weight, Temperature\n",
        "#  2. Categorical Variables-- Categorical variables are variables that represent categories or groups and cannot be measured numerically in a meaningful way. They contain a finite number of possible values, each representing a distinct group or class.\n",
        "#     Example: Gender, colour, ranking"
      ],
      "metadata": {
        "id": "7rMyKN7KtogH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What is feature scaling? How does it help in Machine Learning?\n",
        "#  Feature scaling is the process of transforming the values of numeric variables (features) in a dataset into a common scale. This is especially important when different features have different units or ranges.\n",
        "#  The purpose of feature scaling is to standardize the range of features so that they are all comparable, which improves the performance and convergence of machine learning algorithms.\n",
        "\n",
        "#  How Feature Scaling Helps in Machine Learning\n",
        "#  1. Improves Convergence in Gradient-Based Algorithms\n",
        "#  2. Prevents Certain Features from Dominating\n",
        "#  3. Helps with Regularization\n",
        "#  4. Improves Interpretability"
      ],
      "metadata": {
        "id": "hJp-hzhMuQzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q How do we perform scaling in Python?\n",
        "#  In Python, we can easily perform feature scaling using the scikit-learn library, which provides built-in classes for different scaling techniques like Min-Max Scaling, Standardization, Robust Scaling, and others. Below, I will explain how to use these methods."
      ],
      "metadata": {
        "id": "emykIN9euze2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q What is sklearn.preprocessing?\n",
        "#  sklearn.preprocessing is a module in the scikit-learn library that contains several tools for preprocessing data before applying machine learning models. Preprocessing refers to transforming raw data into a format that can be effectively used by machine learning algorithms.\n",
        "#   It includes techniques for scaling, normalization, encoding categorical data, imputing missing values, and other essential transformations."
      ],
      "metadata": {
        "id": "R7QXKeBtu-93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q How do we split data for model fitting (training and testing) in Python?\n",
        "#  In Python, the most common way to split data for training and testing a model is by using the train_test_split() function from the scikit-learn library.\n",
        "#  This function randomly splits your dataset into two subsets: one for training the model and the other for testing its performance.\n",
        "#   The split is typically done in a way that ensures your model is trained on one part of the data and evaluated on another part to prevent overfitting."
      ],
      "metadata": {
        "id": "11oRDwEvu-rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q Explain data encoding?\n",
        "#  Data encoding is the process of converting categorical data (non-numeric data) into a numerical format that can be understood by machine learning models.\n",
        "#   Machine learning algorithms generally require numerical input, so encoding categorical variables (like strings or labels) into numbers is a crucial step in the preprocessing pipeline.\n"
      ],
      "metadata": {
        "id": "b_SKLh21vbY0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}